{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-QtRX1LkYniv"
   },
   "source": [
    "\n",
    "\n",
    "# Reinforcement Learning for Tasks with continuous action spaces \n",
    " > using:  PPO, LSTMs and multiprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "MMOICRO0awAO",
    "outputId": "883d82de-e78d-4b27-9f3e-1af44d3a712f"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import distributions\n",
    "import gym\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch import optim\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import time\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CartPole-V1 Example:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gPwXKslPzbEH"
   },
   "source": [
    "## Parameters :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rVC7JQaYzbEI",
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "ENV = \"CartPole-v1\" \n",
    "\n",
    "SCALE_REWARD:         float = 0.01\n",
    "MIN_REWARD:           float = -1000.\n",
    "HIDDEN_SIZE:          float = 128\n",
    "BATCH_SIZE:           int   = 512\n",
    "DISCOUNT:             float = 0.99\n",
    "GAE_LAMBDA:           float = 0.95\n",
    "PPO_CLIP:             float = 0.2\n",
    "PPO_EPOCHS:           int   = 10\n",
    "MAX_GRAD_NORM:        float = 1.\n",
    "ENTROPY_FACTOR:       float = 0.\n",
    "ACTOR_LEARNING_RATE:  float = 1e-4\n",
    "CRITIC_LEARNING_RATE: float = 1e-4\n",
    "RECURRENT_SEQ_LEN:    int = 8\n",
    "RECURRENT_LAYERS:     int = 1    \n",
    "ROLLOUT_STEPS:        int = 2048\n",
    "PARALLEL_ROLLOUTS:    int = 8\n",
    "PATIENCE:             int = 2\n",
    "TRAINABLE_STD_DEV:    bool = False \n",
    "INIT_LOG_STD_DEV:     float = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q0dh7BVXzbEK"
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class HyperParameters():\n",
    "    scale_reward:         float = SCALE_REWARD\n",
    "    min_reward:           float = MIN_REWARD\n",
    "    hidden_size:          float = HIDDEN_SIZE\n",
    "    batch_size:           int   = BATCH_SIZE\n",
    "    discount:             float = DISCOUNT\n",
    "    gae_lambda:           float = GAE_LAMBDA\n",
    "    ppo_clip:             float = PPO_CLIP\n",
    "    ppo_epochs:           int   = PPO_EPOCHS\n",
    "    max_grad_norm:        float = MAX_GRAD_NORM\n",
    "    entropy_factor:       float = ENTROPY_FACTOR\n",
    "    actor_learning_rate:  float = ACTOR_LEARNING_RATE\n",
    "    critic_learning_rate: float = CRITIC_LEARNING_RATE\n",
    "    recurrent_seq_len:    int = RECURRENT_SEQ_LEN\n",
    "    recurrent_layers:     int = RECURRENT_LAYERS \n",
    "    rollout_steps:        int = ROLLOUT_STEPS\n",
    "    parallel_rollouts:    int = PARALLEL_ROLLOUTS\n",
    "    patience:             int = PATIENCE\n",
    "    trainable_std_dev:    bool = TRAINABLE_STD_DEV\n",
    "    init_log_std_dev:     float = INIT_LOG_STD_DEV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define environment specific hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "luOSwtG2zbEM"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_count: 16.0\n"
     ]
    }
   ],
   "source": [
    "hp = HyperParameters(parallel_rollouts=32, rollout_steps=512, batch_size=128, recurrent_seq_len=8)\n",
    "batch_count = hp.parallel_rollouts * hp.rollout_steps / hp.recurrent_seq_len / hp.batch_size\n",
    "print(f\"batch_count: {batch_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for discounts, advantages, start and stop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WZfBuuV4zbES"
   },
   "outputs": [],
   "source": [
    "def calc_discounted_return(rewards, discount, final_value):\n",
    "    # Calculate discounted returns based on rewards and discount factor\n",
    "    seq_len = len(rewards)\n",
    "    discounted_returns = torch.zeros(seq_len)\n",
    "    discounted_returns[-1] = rewards[-1] + discount * final_value\n",
    "    for i in range(seq_len - 2, -1 , -1):\n",
    "        discounted_returns[i] = rewards[i] + discount * discounted_returns[i + 1]\n",
    "    return discounted_returns\n",
    "\n",
    "def compute_advantages(rewards, values, discount, gae_lambda):\n",
    "    #Compute General Advantage.\n",
    "    deltas = rewards + discount * values[1:] - values[:-1]\n",
    "    seq_len = len(rewards)\n",
    "    advs = torch.zeros(seq_len + 1)\n",
    "    multiplier = discount * gae_lambda\n",
    "    for i in range(seq_len - 1, -1 , -1):\n",
    "        advs[i] = advs[i + 1] * multiplier  + deltas[i]\n",
    "    return advs[:-1]\n",
    "\n",
    "def get_env_space():\n",
    "    # Return obsvervation dimensions, action dimensions and whether or not action space is continuous\n",
    "    env = gym.make(ENV)\n",
    "    continuous_action_space = type(env.action_space) is gym.spaces.box.Box\n",
    "    if continuous_action_space:\n",
    "        action_dim =  env.action_space.shape[0]\n",
    "    else:\n",
    "        action_dim = env.action_space.n \n",
    "    obsv_dim= env.observation_space.shape[0] \n",
    "    return obsv_dim, action_dim, continuous_action_space\n",
    "\n",
    "def start():\n",
    "    iteration = 0\n",
    "    # create actor and critic\n",
    "    obsv_dim, action_dim, continuous_action_space = get_env_space()\n",
    "    actor = Actor(obsv_dim,\n",
    "                  action_dim,\n",
    "                  continuous_action_space=continuous_action_space,\n",
    "                  trainable_std_dev=hp.trainable_std_dev,\n",
    "                  init_log_std_dev=hp.init_log_std_dev)\n",
    "    critic = Critic(obsv_dim)\n",
    "    \n",
    "    # create optimizers\n",
    "    actor_optimizer = optim.AdamW(actor.parameters(), lr=hp.actor_learning_rate)\n",
    "    critic_optimizer = optim.AdamW(critic.parameters(), lr=hp.critic_learning_rate)\n",
    "    \n",
    "    stop_conditions = StopConditions()\n",
    "    return actor, critic, actor_optimizer, critic_optimizer, iteration, stop_conditions\n",
    "            \n",
    "@dataclass\n",
    "class StopConditions():\n",
    "    # Store parameters and variables used to stop training\n",
    "    best_reward: float = -1e6\n",
    "    fail_to_improve_count: int = 0\n",
    "    max_iterations: int = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pw-loQJ9zbEb"
   },
   "source": [
    "## LSTM Actor and Critic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_jcQSk0-zbEb"
   },
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, continuous_action_space, trainable_std_dev, init_log_std_dev=None):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(state_dim, hp.hidden_size, num_layers=hp.recurrent_layers)\n",
    "        self.layer_hidden = nn.Linear(hp.hidden_size, hp.hidden_size)\n",
    "        self.layer_policy_logits = nn.Linear(hp.hidden_size, action_dim)\n",
    "        self.action_dim = action_dim\n",
    "        self.continuous_action_space = continuous_action_space \n",
    "        self.log_std_dev = nn.Parameter(init_log_std_dev * torch.ones((action_dim), dtype=torch.float), requires_grad=trainable_std_dev)\n",
    "        self.covariance_eye = torch.eye(self.action_dim).unsqueeze(0)\n",
    "        self.hidden_cell = None\n",
    "        \n",
    "    def get_init_state(self, batch_size, device):\n",
    "        self.hidden_cell = (torch.zeros(hp.recurrent_layers, batch_size, hp.hidden_size).to(device),\n",
    "                            torch.zeros(hp.recurrent_layers, batch_size,hp.hidden_size).to(device))\n",
    "        \n",
    "    def forward(self, state, terminal=None):\n",
    "        batch_size = state.shape[1]\n",
    "        device = state.device\n",
    "        if self.hidden_cell is None or batch_size != self.hidden_cell[0].shape[1]:\n",
    "            self.get_init_state(batch_size, device)\n",
    "        if terminal is not None:\n",
    "            self.hidden_cell = [value * (1. - terminal).reshape(1, batch_size, 1) for value in self.hidden_cell]\n",
    "        _, self.hidden_cell = self.lstm(state, self.hidden_cell)\n",
    "        hidden_out = F.elu(self.layer_hidden(self.hidden_cell[0][-1]))\n",
    "        policy_logits_out = self.layer_policy_logits(hidden_out)\n",
    "        if self.continuous_action_space:\n",
    "            cov_matrix = self.covariance_eye.to(device).expand(batch_size, self.action_dim, self.action_dim) * torch.exp(self.log_std_dev.to(device))\n",
    "            policy_dist = torch.distributions.multivariate_normal.MultivariateNormal(policy_logits_out.to(\"cpu\"), cov_matrix.to(\"cpu\"))\n",
    "        else:\n",
    "            policy_dist = distributions.Categorical(F.softmax(policy_logits_out, dim=1).to(\"cpu\"))\n",
    "        return policy_dist\n",
    "    \n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_dim):\n",
    "        super().__init__()\n",
    "        self.layer_lstm = nn.LSTM(state_dim, hp.hidden_size, num_layers=hp.recurrent_layers)\n",
    "        self.layer_hidden = nn.Linear(hp.hidden_size, hp.hidden_size)\n",
    "        self.layer_value = nn.Linear(hp.hidden_size, 1)\n",
    "        self.hidden_cell = None\n",
    "        \n",
    "    def get_init_state(self, batch_size, device):\n",
    "        self.hidden_cell = (torch.zeros(hp.recurrent_layers, batch_size, hp.hidden_size).to(device),\n",
    "                            torch.zeros(hp.recurrent_layers, batch_size, hp.hidden_size).to(device))\n",
    "    \n",
    "    def forward(self, state, terminal=None):\n",
    "        batch_size = state.shape[1]\n",
    "        device = state.device\n",
    "        if self.hidden_cell is None or batch_size != self.hidden_cell[0].shape[1]:\n",
    "            self.get_init_state(batch_size, device)\n",
    "        if terminal is not None:\n",
    "            self.hidden_cell = [value * (1. - terminal).reshape(1, batch_size, 1) for value in self.hidden_cell]\n",
    "        _, self.hidden_cell = self.layer_lstm(state, self.hidden_cell)\n",
    "        hidden_out = F.elu(self.layer_hidden(self.hidden_cell[0][-1]))\n",
    "        value_out = self.layer_value(hidden_out)\n",
    "        return value_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FxHUaJRezbEd"
   },
   "source": [
    "## Get trajectories from environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8X_xYO-_cLUe"
   },
   "outputs": [],
   "source": [
    "_MIN_REWARD_VALUES = torch.full([hp.parallel_rollouts], hp.min_reward)\n",
    "\n",
    "def gather_trajectories(input_data):    \n",
    "    # get inputs\n",
    "    env = input_data[\"env\"]\n",
    "    actor = input_data[\"actor\"]\n",
    "    critic = input_data[\"critic\"]\n",
    "    \n",
    "    # Initialise variables\n",
    "    obsv = env.reset()\n",
    "    trajectory_data = {\"states\": [],\n",
    "                 \"actions\": [],\n",
    "                 \"action_probabilities\": [],\n",
    "                 \"rewards\": [],\n",
    "                 \"true_rewards\": [],\n",
    "                 \"values\": [],\n",
    "                 \"terminals\": [],\n",
    "                 \"actor_hidden_states\": [],\n",
    "                 \"actor_cell_states\": [],\n",
    "                 \"critic_hidden_states\": [],\n",
    "                 \"critic_cell_states\": []}\n",
    "    terminal = torch.ones(hp.parallel_rollouts) \n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Reset actor and critic state\n",
    "        actor.get_init_state(hp.parallel_rollouts, GATHER_DEVICE)\n",
    "        critic.get_init_state(hp.parallel_rollouts, GATHER_DEVICE)\n",
    "        # additional step to collect the state and value for the final state\n",
    "        for i in range(hp.rollout_steps):\n",
    "            \n",
    "            trajectory_data[\"actor_hidden_states\"].append(actor.hidden_cell[0].squeeze(0).cpu())\n",
    "            trajectory_data[\"actor_cell_states\"].append(actor.hidden_cell[1].squeeze(0).cpu())\n",
    "            trajectory_data[\"critic_hidden_states\"].append(critic.hidden_cell[0].squeeze(0).cpu())\n",
    "            trajectory_data[\"critic_cell_states\"].append(critic.hidden_cell[1].squeeze(0).cpu())\n",
    "            \n",
    "            # Choose next action \n",
    "            state = torch.tensor(obsv, dtype=torch.float32)\n",
    "            trajectory_data[\"states\"].append(state)\n",
    "            value = critic(state.unsqueeze(0).to(GATHER_DEVICE), terminal.to(GATHER_DEVICE))\n",
    "            trajectory_data[\"values\"].append( value.squeeze(1).cpu())\n",
    "            action_dist = actor(state.unsqueeze(0).to(GATHER_DEVICE), terminal.to(GATHER_DEVICE))\n",
    "            action = action_dist.sample().reshape(hp.parallel_rollouts, -1)\n",
    "            if not actor.continuous_action_space:\n",
    "                action = action.squeeze(1)\n",
    "            trajectory_data[\"actions\"].append(action.cpu())\n",
    "            trajectory_data[\"action_probabilities\"].append(action_dist.log_prob(action).cpu())\n",
    "\n",
    "            # environment step\n",
    "            action_np = action.cpu().numpy()\n",
    "            obsv, reward, done, _ = env.step(action_np)\n",
    "            terminal = torch.tensor(done).float()\n",
    "            transformed_reward = hp.scale_reward * torch.max(_MIN_REWARD_VALUES, torch.tensor(reward).float())\n",
    "                                                             \n",
    "            trajectory_data[\"rewards\"].append(transformed_reward)\n",
    "            trajectory_data[\"true_rewards\"].append(torch.tensor(reward).float())\n",
    "            trajectory_data[\"terminals\"].append(terminal)\n",
    "    \n",
    "        # Compute final value to allow for incomplete episodes\n",
    "        state = torch.tensor(obsv, dtype=torch.float32)\n",
    "        value = critic(state.unsqueeze(0).to(GATHER_DEVICE), terminal.to(GATHER_DEVICE))\n",
    "        # Future value for terminal episodes is 0.\n",
    "        trajectory_data[\"values\"].append(value.squeeze(1).cpu() * (1 - terminal))\n",
    "\n",
    "    # Combine into tensors\n",
    "    trajectory_tensors = {key: torch.stack(value) for key, value in trajectory_data.items()}\n",
    "    return trajectory_tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AmCh6eiUcU-d"
   },
   "outputs": [],
   "source": [
    "def split_trajectories_episodes(trajectory_tensors):\n",
    "\n",
    "    states_episodes, actions_episodes, action_probabilities_episodes = [], [], []\n",
    "    rewards_episodes, terminal_rewards_episodes, terminals_episodes, values_episodes = [], [], [], []\n",
    "    policy_hidden_episodes, policy_cell_episodes, critic_hidden_episodes, critic_cell_episodes = [], [], [], []\n",
    "    len_episodes = []\n",
    "    trajectory_episodes = {key: [] for key in trajectory_tensors.keys()}\n",
    "    for i in range(hp.parallel_rollouts):\n",
    "        terminals_tmp = trajectory_tensors[\"terminals\"].clone()\n",
    "        terminals_tmp[0, i] = 1\n",
    "        terminals_tmp[-1, i] = 1\n",
    "        split_points = (terminals_tmp[:, i] == 1).nonzero() + 1\n",
    "\n",
    "        split_lens = split_points[1:] - split_points[:-1]\n",
    "        split_lens[0] += 1\n",
    "        \n",
    "        len_episode = [split_len.item() for split_len in split_lens]\n",
    "        len_episodes += len_episode\n",
    "        for key, value in trajectory_tensors.items():\n",
    "            # Value includes additional step\n",
    "            if key == \"values\":\n",
    "                value_split = list(torch.split(value[:, i], len_episode[:-1] + [len_episode[-1] + 1]))\n",
    "                # Append extra 0 to values to represent no future reward\n",
    "                for j in range(len(value_split) - 1):\n",
    "                    value_split[j] = torch.cat((value_split[j], torch.zeros(1)))\n",
    "                trajectory_episodes[key] += value_split\n",
    "            else:\n",
    "                trajectory_episodes[key] += torch.split(value[:, i], len_episode)\n",
    "    return trajectory_episodes, len_episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Obi40GEmcwKR"
   },
   "outputs": [],
   "source": [
    "def pad_and_compute_returns(trajectory_episodes, len_episodes):\n",
    "    # Pad the trajectories up to hp.rollout_steps so they can be combined in a single tensor\n",
    "    # Add advantages and discounted_returns to trajectories\n",
    "    \n",
    "    episode_count = len(len_episodes)\n",
    "    advantages_episodes, discounted_returns_episodes = [], []\n",
    "    padded_trajectories = {key: [] for key in trajectory_episodes.keys()}\n",
    "    padded_trajectories[\"advantages\"] = []\n",
    "    padded_trajectories[\"discounted_returns\"] = []\n",
    "\n",
    "    for i in range(episode_count):\n",
    "        single_padding = torch.zeros(hp.rollout_steps - len_episodes[i])\n",
    "        for key, value in trajectory_episodes.items():\n",
    "            if value[i].ndim > 1:\n",
    "                padding = torch.zeros(hp.rollout_steps - len_episodes[i], value[0].shape[1], dtype=value[i].dtype)\n",
    "            else:\n",
    "                padding = torch.zeros(hp.rollout_steps - len_episodes[i], dtype=value[i].dtype)\n",
    "            padded_trajectories[key].append(torch.cat((value[i], padding)))\n",
    "        padded_trajectories[\"advantages\"].append(torch.cat((compute_advantages(rewards=trajectory_episodes[\"rewards\"][i],\n",
    "                                                        values=trajectory_episodes[\"values\"][i],\n",
    "                                                        discount=DISCOUNT,\n",
    "                                                        gae_lambda=GAE_LAMBDA), single_padding)))\n",
    "        padded_trajectories[\"discounted_returns\"].append(torch.cat((calc_discounted_return(rewards=trajectory_episodes[\"rewards\"][i],\n",
    "                                                                    discount=DISCOUNT,\n",
    "                                                                    final_value=trajectory_episodes[\"values\"][i][-1]), single_padding)))\n",
    "    return_val = {k: torch.stack(v) for k, v in padded_trajectories.items()} \n",
    "    return_val[\"seq_len\"] = torch.tensor(len_episodes)\n",
    "    \n",
    "    return return_val "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ogbnxw62zbEh"
   },
   "source": [
    "## Create Training dataset from trajectories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oXGqWP8jJaYk"
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TrajectorBatch():\n",
    "    # Dataclass for storing data batch\n",
    "\n",
    "    states: torch.tensor\n",
    "    actions: torch.tensor\n",
    "    action_probabilities: torch.tensor\n",
    "    advantages: torch.tensor\n",
    "    discounted_returns: torch.tensor\n",
    "    batch_size: torch.tensor\n",
    "    actor_hidden_states: torch.tensor\n",
    "    actor_cell_states: torch.tensor\n",
    "    critic_hidden_states: torch.tensor\n",
    "    critic_cell_states: torch.tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_CF49auPzbEi"
   },
   "outputs": [],
   "source": [
    "class TrajectoryDataset():\n",
    "    # Dataset for producing training batches from trajectories\n",
    "\n",
    "    def __init__(self, trajectories, batch_size, device, batch_len):\n",
    "        \n",
    "        # Combine multiple trajectories into\n",
    "        self.trajectories = {key: value.to(device) for key, value in trajectories.items()}\n",
    "        self.batch_len = batch_len \n",
    "        truncated_seq_len = torch.clamp(trajectories[\"seq_len\"] - batch_len + 1, 0, hp.rollout_steps)\n",
    "        self.cumsum_seq_len =  np.cumsum(np.concatenate( (np.array([0]), truncated_seq_len.numpy())))\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "    def __iter__(self):\n",
    "        self.valid_idx = np.arange(self.cumsum_seq_len[-1])\n",
    "        self.batch_count = 0\n",
    "        return self\n",
    "        \n",
    "    def __next__(self):\n",
    "        if self.batch_count * self.batch_size >= math.ceil(self.cumsum_seq_len[-1] / self.batch_len):\n",
    "            raise StopIteration\n",
    "        else:\n",
    "            actual_batch_size = min(len(self.valid_idx), self.batch_size) \n",
    "            start_idx = np.random.choice(self.valid_idx, size=actual_batch_size, replace=False )\n",
    "            self.valid_idx = np.setdiff1d(self.valid_idx, start_idx)\n",
    "            eps_idx = np.digitize(start_idx, bins = self.cumsum_seq_len, right=False) - 1\n",
    "            seq_idx = start_idx - self.cumsum_seq_len[eps_idx]\n",
    "            series_idx = np.linspace(seq_idx, seq_idx + self.batch_len - 1, num=self.batch_len, dtype=np.int64)\n",
    "            self.batch_count += 1\n",
    "            return TrajectorBatch(**{key: value[eps_idx, series_idx]for key, value\n",
    "                                     in self.trajectories.items() if key in TrajectorBatch.__dataclass_fields__.keys()},\n",
    "                                  batch_size=actual_batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2_wzDQqSzbEm"
   },
   "source": [
    "## PPO Training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qxvWtBAyzbEn"
   },
   "outputs": [],
   "source": [
    "def train_model(actor, critic, actor_optimizer, critic_optimizer, iteration, stop_conditions):\n",
    "    \n",
    "    # Vector environment manages multiple instances of the environment, this environment automatically resets\n",
    "    env = gym.vector.make(ENV, hp.parallel_rollouts, asynchronous=False)\n",
    "\n",
    "    while iteration < stop_conditions.max_iterations:      \n",
    "\n",
    "        actor = actor.to(GATHER_DEVICE)\n",
    "        critic = critic.to(GATHER_DEVICE)\n",
    "        start_gather_time = time.time()\n",
    "\n",
    "        # Get trajectories\n",
    "        input_data = {\"env\": env, \"actor\": actor, \"critic\": critic, \"discount\": hp.discount,\n",
    "                      \"gae_lambda\": hp.gae_lambda}\n",
    "        trajectory_tensors = gather_trajectories(input_data)\n",
    "        trajectory_episodes, len_episodes = split_trajectories_episodes(trajectory_tensors)\n",
    "        trajectories = pad_and_compute_returns(trajectory_episodes, len_episodes)\n",
    "\n",
    "        # Calculate mean reward\n",
    "        complete_episode_count = trajectories[\"terminals\"].sum().item()\n",
    "        terminal_episodes_rewards = (trajectories[\"terminals\"].sum(axis=1) * trajectories[\"true_rewards\"].sum(axis=1)).sum()\n",
    "        mean_reward =  terminal_episodes_rewards / (complete_episode_count)\n",
    "\n",
    "        # Check stop conditions\n",
    "        if mean_reward > stop_conditions.best_reward:\n",
    "            stop_conditions.best_reward = mean_reward\n",
    "            stop_conditions.fail_to_improve_count = 0\n",
    "        else:\n",
    "            stop_conditions.fail_to_improve_count += 1\n",
    "        if stop_conditions.fail_to_improve_count > hp.patience:\n",
    "            print(f\"Policy has not yielded higher reward for {hp.patience} iterations...  Stopping now.\")\n",
    "            break\n",
    "\n",
    "        trajectory_dataset = TrajectoryDataset(trajectories, batch_size=hp.batch_size,\n",
    "                                        device=TRAIN_DEVICE, batch_len=hp.recurrent_seq_len)\n",
    "        end_gather_time = time.time()\n",
    "        start_train_time = time.time()\n",
    "        \n",
    "        actor = actor.to(TRAIN_DEVICE)\n",
    "        critic = critic.to(TRAIN_DEVICE)\n",
    "\n",
    "        # Train actor and critic\n",
    "        for epoch_idx in range(hp.ppo_epochs): \n",
    "            for batch in trajectory_dataset:\n",
    "\n",
    "                # Get batch \n",
    "                actor.hidden_cell = (batch.actor_hidden_states[:1], batch.actor_cell_states[:1])\n",
    "                \n",
    "                # Update actor\n",
    "                actor_optimizer.zero_grad()\n",
    "                action_dist = actor(batch.states)\n",
    "                action_probabilities = action_dist.log_prob(batch.actions[-1, :].to(\"cpu\")).to(TRAIN_DEVICE)\n",
    "                \n",
    "                # Compute probability ratio from probabilities in logspace\n",
    "                probabilities_ratio = torch.exp(action_probabilities - batch.action_probabilities[-1, :])\n",
    "                surrogate_loss_0 = probabilities_ratio * batch.advantages[-1, :]\n",
    "                surrogate_loss_1 =  torch.clamp(probabilities_ratio, 1. - hp.ppo_clip, 1. + hp.ppo_clip) * batch.advantages[-1, :]\n",
    "                surrogate_loss_2 = action_dist.entropy().to(TRAIN_DEVICE)\n",
    "                actor_loss = -torch.mean(torch.min(surrogate_loss_0, surrogate_loss_1)) - torch.mean(hp.entropy_factor * surrogate_loss_2)\n",
    "                actor_loss.backward() \n",
    "                torch.nn.utils.clip_grad.clip_grad_norm_(actor.parameters(), hp.max_grad_norm)\n",
    "                actor_optimizer.step()\n",
    "\n",
    "                # Update critic\n",
    "                critic_optimizer.zero_grad()\n",
    "                critic.hidden_cell = (batch.critic_hidden_states[:1], batch.critic_cell_states[:1])\n",
    "                values = critic(batch.states)\n",
    "                critic_loss = F.mse_loss(batch.discounted_returns[-1, :], values.squeeze(1))\n",
    "                torch.nn.utils.clip_grad.clip_grad_norm_(critic.parameters(), hp.max_grad_norm)\n",
    "                critic_loss.backward() \n",
    "                critic_optimizer.step()\n",
    "                \n",
    "        end_train_time = time.time()\n",
    "        # provide info\n",
    "        print(f\"Iteration: {iteration},  Mean reward: {mean_reward}, Mean Entropy: {torch.mean(surrogate_loss_2)}, \" +\n",
    "              f\"complete_episode_count: {complete_episode_count}, Gather time: {end_gather_time - start_gather_time:.2f}s, \" +\n",
    "              f\"Train time: {end_train_time - start_train_time:.2f}s\")\n",
    "\n",
    "        # save metrics\n",
    "        writer.add_scalar(\"complete_episode_count\", complete_episode_count, iteration)\n",
    "        writer.add_scalar(\"total_reward\", mean_reward , iteration)\n",
    "        writer.add_scalar(\"actor_loss\", actor_loss, iteration)\n",
    "        writer.add_scalar(\"critic_loss\", critic_loss, iteration)\n",
    "        writer.add_scalar(\"policy_entropy\", torch.mean(surrogate_loss_2), iteration)\n",
    "        iteration += 1\n",
    "        \n",
    "    return stop_conditions.best_reward "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KQMxee2ZFFau"
   },
   "source": [
    "## Run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f_a8KIvCzbEp"
   },
   "outputs": [],
   "source": [
    "RANDOM_SEED = 0\n",
    "torch.random.manual_seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.set_num_threads(1)\n",
    "TRAIN_DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "GATHER_DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0,  Mean reward: 22.240222930908203, Mean Entropy: 0.6839659214019775, complete_episode_count: 716.0, Gather time: 5.02s, Train time: 7.37s\n",
      "Iteration: 1,  Mean reward: 24.69730567932129, Mean Entropy: 0.6644285917282104, complete_episode_count: 631.0, Gather time: 3.94s, Train time: 7.55s\n",
      "Iteration: 2,  Mean reward: 31.949289321899414, Mean Entropy: 0.619078516960144, complete_episode_count: 493.0, Gather time: 3.44s, Train time: 7.93s\n",
      "Iteration: 3,  Mean reward: 45.40412902832031, Mean Entropy: 0.5781623125076294, complete_episode_count: 339.0, Gather time: 3.41s, Train time: 9.39s\n",
      "Iteration: 4,  Mean reward: 86.8922119140625, Mean Entropy: 0.5250725746154785, complete_episode_count: 167.0, Gather time: 2.77s, Train time: 9.64s\n",
      "Iteration: 5,  Mean reward: 151.0689697265625, Mean Entropy: 0.508776068687439, complete_episode_count: 87.0, Gather time: 3.91s, Train time: 10.33s\n",
      "Iteration: 6,  Mean reward: 237.6999969482422, Mean Entropy: 0.534351646900177, complete_episode_count: 50.0, Gather time: 2.71s, Train time: 10.00s\n",
      "Iteration: 7,  Mean reward: 277.95123291015625, Mean Entropy: 0.5421738028526306, complete_episode_count: 41.0, Gather time: 2.41s, Train time: 9.74s\n",
      "Iteration: 8,  Mean reward: 410.6666564941406, Mean Entropy: 0.5065659284591675, complete_episode_count: 33.0, Gather time: 2.85s, Train time: 13.45s\n",
      "Iteration: 9,  Mean reward: 470.9375, Mean Entropy: 0.5006434917449951, complete_episode_count: 32.0, Gather time: 2.72s, Train time: 11.65s\n",
      "Iteration: 10,  Mean reward: 448.6875, Mean Entropy: 0.5478019118309021, complete_episode_count: 32.0, Gather time: 2.87s, Train time: 14.24s\n",
      "Iteration: 11,  Mean reward: 493.3125, Mean Entropy: 0.5265197157859802, complete_episode_count: 32.0, Gather time: 3.46s, Train time: 11.68s\n",
      "Iteration: 12,  Mean reward: 478.46875, Mean Entropy: 0.5223309993743896, complete_episode_count: 32.0, Gather time: 3.04s, Train time: 10.80s\n",
      "Iteration: 13,  Mean reward: 487.0, Mean Entropy: 0.4859425127506256, complete_episode_count: 32.0, Gather time: 2.46s, Train time: 10.88s\n",
      "Iteration: 14,  Mean reward: 500.0, Mean Entropy: 0.5144342184066772, complete_episode_count: 32.0, Gather time: 2.78s, Train time: 9.58s\n",
      "Iteration: 15,  Mean reward: 487.25, Mean Entropy: 0.4528152048587799, complete_episode_count: 32.0, Gather time: 2.42s, Train time: 9.62s\n",
      "Iteration: 16,  Mean reward: 494.6875, Mean Entropy: 0.5078219175338745, complete_episode_count: 32.0, Gather time: 2.44s, Train time: 9.64s\n",
      "Policy has not yielded higher reward for 2 iterations...  Stopping now.\n"
     ]
    }
   ],
   "source": [
    "writer = SummaryWriter()\n",
    "actor, critic, actor_optimizer, critic_optimizer, iteration, stop_conditions = start()\n",
    "score = train_model(actor, critic, actor_optimizer, critic_optimizer, iteration, stop_conditions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "recurrent_ppo.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
