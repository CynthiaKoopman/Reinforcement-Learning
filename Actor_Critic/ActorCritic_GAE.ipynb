{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation of Actor-critic with GAE\n",
    "### *heavily inpsired by OpenAI code\n",
    "This code makes use of a Memory to store various values, this makes it easier to pass the values, but also for the GAE calculation. For actor-critic updates the episode does not have to end, and if working with env like cartpole, a batch can contain several episodes. This code also does not share weights between actor and critic contrary to the \"simple\" implementation found in this repository. \n",
    "\n",
    "Also please note that parameters have not been optimized. The parameters and environment are chosen for simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.signal\n",
    "import gym\n",
    "from torch.optim import Adam\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions.categorical import Categorical\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPActorCritic(nn.Module):\n",
    "\n",
    "\n",
    "    def __init__(self, observation_space, action_space, \n",
    "                 hidden_sizes = (64,64), activation=nn.Tanh):\n",
    "        super().__init__()\n",
    "\n",
    "        obs_dim = observation_space.shape[0]\n",
    "\n",
    "        # policy builder depends on action space\n",
    "        \"\"\"if isinstance(action_space, Box):\n",
    "            self.pi = MLPGaussianActor(obs_dim, action_space.shape[0], hidden_sizes, activation)\n",
    "        elif isinstance(action_space, Discrete):\"\"\"\n",
    "        self.pi = MLPCategoricalActor(obs_dim, action_space.n, hidden_sizes, activation) #actor\n",
    "\n",
    "        # build value function\n",
    "        self.v  = MLPCritic(obs_dim, hidden_sizes, activation) #critic\n",
    "\n",
    "    def step(self, obs):\n",
    "        with torch.no_grad():\n",
    "            pi = self.pi._distribution(obs)\n",
    "            a = pi.sample()\n",
    "            logp_a = self.pi._log_prob_from_distribution(pi, a)\n",
    "            v = self.v(obs)\n",
    "        return a.numpy(), v.numpy(), logp_a.numpy()\n",
    "\n",
    "    def act(self, obs):\n",
    "        return self.step(obs)[0]\n",
    "    \n",
    "class MLPCritic(nn.Module):\n",
    "\n",
    "    def __init__(self, obs_dim, hidden_sizes, activation):\n",
    "        super().__init__()\n",
    "        self.v_net = mlp([obs_dim] + list(hidden_sizes) + [1], activation)\n",
    "\n",
    "    def forward(self, obs):\n",
    "        return torch.squeeze(self.v_net(obs), -1)\n",
    "    \n",
    "class MLPCategoricalActor(nn.Module):\n",
    "    \n",
    "    def __init__(self, obs_dim, act_dim, hidden_sizes, activation):\n",
    "        super().__init__()\n",
    "        self.logits_net = mlp([obs_dim] + list(hidden_sizes) + [act_dim], activation)\n",
    "\n",
    "    def _distribution(self, obs):\n",
    "        logits = self.logits_net(obs)\n",
    "        return Categorical(logits=logits)\n",
    "\n",
    "    def _log_prob_from_distribution(self, pi, act):\n",
    "        return pi.log_prob(act)\n",
    "    \n",
    "    def forward(self, obs, act=None):\n",
    "        # Produce action distributions for given observations, and \n",
    "        # optionally compute the log likelihood of given actions under\n",
    "        # those distributions.\n",
    "        pi = self._distribution(obs)\n",
    "        logp_a = None\n",
    "        if act is not None:\n",
    "            logp_a = self._log_prob_from_distribution(pi, act)\n",
    "        return pi, logp_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp(sizes, activation, output_activation=nn.Identity):\n",
    "    layers = []\n",
    "    for j in range(len(sizes)-1):\n",
    "        act = activation if j < len(sizes)-2 else output_activation\n",
    "        layers += [nn.Linear(sizes[j], sizes[j+1]), act()]\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "def combined_shape(length, shape=None):\n",
    "    if shape is None:\n",
    "        return (length,)\n",
    "    return (length, shape) if np.isscalar(shape) else (length, *shape)\n",
    "\n",
    "def discount_cumsum(x, discount):\n",
    "    return scipy.signal.lfilter([1], [1, float(-discount)], x[::-1], axis=0)[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory:\n",
    "    # make memory\n",
    "    def __init__(self, obs_dim, act_dim, size, gamma, lambda_gae):\n",
    "        # save obs, act, adv, rew, rewtg\n",
    "        self.obs_buf = np.zeros(combined_shape(size, obs_dim), dtype=np.float32)\n",
    "        self.act_buf = np.zeros(combined_shape(size, act_dim), dtype=np.float32)\n",
    "        self.adv_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.rew_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.rewtg_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.val_buf = np.zeros(size, dtype=np.float32)\n",
    "        # define gamma and lambda\n",
    "        self.gamma, self.lambda_gae = gamma, lambda_gae\n",
    "        #define pointers to keep track of beginning and ending of episode (used for GAE)\n",
    "        self.ptr, self.path_start_idx, self.max_size = 0, 0, size\n",
    "        \n",
    "    # make functions to: store, calculate gae and get data-----------------------------------------------------------\n",
    "    def store(self, obs, act, rew, val):#, logp):\n",
    "        assert self.ptr < self.max_size     # make sure that there is enough room in memory\n",
    "        self.obs_buf[self.ptr] = obs\n",
    "        self.act_buf[self.ptr] = act\n",
    "        self.rew_buf[self.ptr] = rew\n",
    "        self.val_buf[self.ptr] = val\n",
    "        self.ptr += 1 # move pointer\n",
    "        \n",
    "    def GAE(self, last_val=0):\n",
    "        # take only steps that are part of that episode\n",
    "        path_slice = slice(self.path_start_idx, self.ptr) # the indices\n",
    "        rews = np.append(self.rew_buf[path_slice], last_val)\n",
    "        vals = np.append(self.val_buf[path_slice], last_val)\n",
    "        \n",
    "        # GAE-Lambda advantage calculation\n",
    "        deltas = rews[:-1] + self.gamma * vals[1:] - vals[:-1]\n",
    "        self.adv_buf[path_slice] = discount_cumsum(deltas, self.gamma * self.lambda_gae)\n",
    "        \n",
    "        # reward to go = targets for value function\n",
    "        self.rewtg_buf[path_slice] = discount_cumsum(rews, self.gamma)[:-1] \n",
    "        \n",
    "        self.path_start_idx = self.ptr # reset pointer\n",
    "        \n",
    "    def get(self): # get all data in memory\n",
    "        assert self.ptr == self.max_size   \n",
    "        self.ptr, self.path_start_idx = 0, 0 # reset pointers\n",
    "        \n",
    "        # advantage normalization (usually mean&std of all parallell workers)\n",
    "        self.adv_buf = (self.adv_buf - np.mean(self.adv_buf)) / np.std(self.adv_buf)\n",
    "        \n",
    "        # return data in seperate tensors\n",
    "        data = dict(obs=self.obs_buf, act=self.act_buf, rewtg=self.rewtg_buf,\n",
    "                    adv=self.adv_buf)\n",
    "        return {k: torch.as_tensor(v, dtype=torch.float32) for k,v in data.items()}\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vpg(env_fn, actor_critic=MLPActorCritic, hidden_sizes=(64,64), gamma=0.99, \n",
    "        seed=0, steps_per_epoch=200, epochs=2, pi_lr=3e-4, vf_lr=1e-3, lambda_gae=0.97, \n",
    "        max_ep_len=200, activation=nn.Tanh):\n",
    "    \n",
    "    # Random seed\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # Instantiate environment\n",
    "    env = env_fn()\n",
    "    obs_dim = env.observation_space.shape\n",
    "    act_dim = env.action_space.shape\n",
    "    \n",
    "    # Create actor-critic module\n",
    "    ac = actor_critic(env.observation_space, env.action_space, hidden_sizes, activation)\n",
    "    \n",
    "    # Set up optimizers for policy and value function\n",
    "    pi_optimizer = Adam(ac.pi.parameters(), lr=pi_lr)\n",
    "    vf_optimizer = Adam(ac.v.parameters(), lr=vf_lr)\n",
    "    \n",
    "    # Set up memory\n",
    "    buf = Memory(obs_dim, act_dim, steps_per_epoch, gamma, lambda_gae)\n",
    "    \n",
    "    #--------------------------------------------------------------------------------------------------------------\n",
    "    # Set up function for computing VPG policy loss\n",
    "    def compute_loss_pi(data):\n",
    "        obs, act, adv = data['obs'], data['act'], data['adv']\n",
    "\n",
    "        # Policy loss\n",
    "        pi, logp = ac.pi(obs, act)\n",
    "        loss_pi = -(logp * adv).mean()\n",
    "\n",
    "        return loss_pi\n",
    "    \n",
    "    # Set up function for computing value loss\n",
    "    def compute_loss_v(data):\n",
    "        obs, rewtg = data['obs'], data['rewtg']\n",
    "        return ((ac.v(obs) - rewtg)**2).mean() # (values - reward to go) MSE\n",
    "    \n",
    "\n",
    "    # Set up function for updating pi and v\n",
    "    def update():\n",
    "        data = buf.get()\n",
    "\n",
    "        # Train policy with a single step of gradient descent\n",
    "        pi_optimizer.zero_grad()\n",
    "        loss_pi = compute_loss_pi(data)\n",
    "        loss_pi.backward()\n",
    "        pi_optimizer.step()\n",
    "\n",
    "        # Value function learning\n",
    "        vf_optimizer.zero_grad()\n",
    "        loss_v = compute_loss_v(data)\n",
    "        loss_v.backward()\n",
    "        vf_optimizer.step()\n",
    "        \n",
    "    #--------------------------------------------------------------------------------------------------------------    \n",
    "    # Prepare for interaction with environment\n",
    "    o = env.reset()\n",
    "    ep_len = 0\n",
    "    ep_rew = 0\n",
    "    all_ep_rew, avg_rew = [], []\n",
    "\n",
    "    # Main loop: collect experience in env and update/log each epoch\n",
    "    for epoch in range(epochs):\n",
    "        for t in range(steps_per_epoch):\n",
    "            a, v, logp = ac.step(torch.as_tensor(o, dtype=torch.float32)) # get action based on initial obs\n",
    "\n",
    "            next_o, r, d, _ = env.step(a) # get next obs\n",
    "            ep_len += 1\n",
    "            ep_rew += r\n",
    "            \n",
    "            # save and log\n",
    "            buf.store(o, a, r, v) #, logp)\n",
    "            \n",
    "            # Update obs (critical!)\n",
    "            o = next_o\n",
    "            \n",
    "            # keep track of where you are in episode/batch\n",
    "            timeout = ep_len == max_ep_len #end of episode\n",
    "            terminal = d or timeout #done or end of episode\n",
    "            epoch_ended = t==steps_per_epoch-1 #end of an epoch when all steps used (when terminal continue)\n",
    "            \n",
    "            if terminal or epoch_ended: # only for these cases need to save next value & reset env\n",
    "                #if epoch_ended and not(terminal):\n",
    "                    #print('Warning: trajectory cut off by epoch at %d steps.'%ep_len, flush=True)\n",
    "                    \n",
    "                # if trajectory didn't reach terminal state, bootstrap value target\n",
    "                if timeout or epoch_ended:\n",
    "                    _, v, _ = ac.step(torch.as_tensor(o, dtype=torch.float32)) # get next value\n",
    "                    all_ep_rew.append(ep_rew)\n",
    "                else:\n",
    "                    v = 0 # if terminal next value is 0\n",
    "                    all_ep_rew.append(ep_rew)\n",
    "                    \n",
    "                buf.GAE(v) # calculate advantages using these values\n",
    "\n",
    "                o, ep_len, ep_rew = env.reset(), 0, 0 # reset the env\n",
    "                \n",
    "        #keep track of average returns of batches/epochs\n",
    "        avg_rew.append(np.mean(all_ep_rew))\n",
    "        if epoch % 100 == 0:\n",
    "            print(\"Avg rewards: \", np.mean(all_ep_rew))\n",
    "            \n",
    "        # Perform policy update, every time step\n",
    "        update()\n",
    "    return avg_rew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg rewards:  16.666666666666668\n",
      "Avg rewards:  33.779264214046826\n",
      "Avg rewards:  48.96467722289891\n",
      "Avg rewards:  61.36595310907237\n",
      "Avg rewards:  71.22557726465364\n",
      "Avg rewards:  80.28846153846153\n",
      "Avg rewards:  85.97997138769671\n",
      "Avg rewards:  92.72486772486772\n",
      "Avg rewards:  97.86194257788638\n",
      "Avg rewards:  101.46396396396396\n",
      "Avg rewards:  105.92592592592592\n",
      "Avg rewards:  109.77068793619142\n",
      "Avg rewards:  112.92900799247766\n",
      "Avg rewards:  115.6958648288128\n",
      "Avg rewards:  118.8295165394402\n",
      "Avg rewards:  121.83441558441558\n",
      "Avg rewards:  124.44617178390983\n",
      "Avg rewards:  127.0825550989914\n",
      "Avg rewards:  128.96527031865378\n",
      "Avg rewards:  131.013094417643\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "result = vpg(lambda : gym.make(\"CartPole-v0\"), actor_critic=MLPActorCritic, hidden_sizes=(64,64), gamma=0.99, \n",
    "             seed=0, steps_per_epoch=200, epochs=2000, pi_lr=3e-4, vf_lr=1e-3, lambda_gae=0.97, \n",
    "             max_ep_len=200, activation=nn.Tanh)\n",
    "\n",
    "plt.plot(result)\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"avg reward\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
